import argparse
import datetime
import gzip
import json
import os
import re
import subprocess
import time
from types import SimpleNamespace as NS

from str_analysis.utils.file_utils import file_exists, tee_stdout_and_stderr_to_log_file

# install str-analysis python package
os.system("""
if ! python3 -m pip show str-analysis &> /dev/null
then
    python3 -m pip install --upgrade git+https://github.com/broadinstitute/str-analysis.git
fi
""")


VERSION = 2

def run(command, step_number=None):
    command = re.sub("[ \\t]{2,}", "  ", command)  # remove extra spaces
    if args.dry_run:
        if step_number is not None:
            print(f"STEP #{step_number}: {command}")
        else:
            print(command)

    if not args.dry_run or command.startswith("mkdir"):
        if step_number is None or (
            not (args.only_step is not None and step_number != args.only_step) and
            not (args.start_with_step is not None and step_number < args.start_with_step) and
            not (args.end_with_step is not None and step_number > args.end_with_step)
        ):
            if step_number is not None:
                print(f"\nSTEP #{step_number}: {command}")
            else:
                print(f"\n--- $ {command}")
            subprocess.run(command, shell=True, check=True)

def chdir(d):
    print(f"cd {d}")
    os.chdir(d)



parser = argparse.ArgumentParser()
parser.add_argument("--hg38-reference-fasta", default="hg38.fa", help="Path of hg38 reference genome FASTA file")
parser.add_argument("--gencode-gtf", default="gencode.v46.basic.annotation.gtf.gz", help="Gene annotations GTF file")
parser.add_argument("--output-prefix", default=f"TRExplorer.repeat_catalog_v{VERSION}.hg38")
parser.add_argument("--variation-clusters-output-prefix", default=f"TRExplorer.variation_clusters_and_isolated_TRs_v{VERSION}.hg38")

parser.add_argument("--only-step", type=int, help="Only run this one step")
parser.add_argument("--start-with-step", type=int, help="Start with a specific step number")
parser.add_argument("--end-with-step", type=int, help="End with a specific step number")
parser.add_argument("--tenk10k-annotations", default="gs://tandem-repeat-catalog/v1.0/tenk10k_str_mt_rows.reformatted.tsv.gz",
                    help="Path of the TenK10K annotations data generated by Hope Tanudisastro by running ExpansionHunter on the short-read TenK10K genomes")
parser.add_argument("--skip-tenk10k-annotations", action="store_true",
                    help="Skip adding TenK10K annotations to the catalog")
parser.add_argument("--hprc256-annotations", default="gs://tandem-repeat-catalog/v2.0/hprc_lps.2025_12.grouped_by_locus_and_motif.with_biallelic_histogram.tsv.gz",
                    help="Path of the HPRC256 annotations table generated by Egor Dolzhenko by running TRGT on 256 diverse PacBio 1kGP samples from the HPRC")
parser.add_argument("--skip-hprc256-annotations", action="store_true",
                    help="Skip adding HPRC256 annotations to the catalog")
parser.add_argument("--aou1027-annotations", default="gs://tandem-repeat-catalog/v1.0.1/AoULR_phase1_TRGT_Weisburd_v1.0.1_combined.txt.gz",
                    help="Path of the AoU1027 annotations table shared by Matt Danzi")
parser.add_argument("--skip-aou1027-annotations", action="store_true",
                    help="Skip adding AoU1027 annotations to the catalog")
parser.add_argument("--variation-clusters-bed", default="gs://tandem-repeat-catalog/v2.0/trs_and_vcs_2025_12_01.bed.gz",
                    help="Variation clusters file shared by Egor Dolzhenko")
parser.add_argument("--skip-variation-cluster-annotations", action="store_true",
                    help="Skip adding variation cluster annotations to the catalog")
parser.add_argument("--timestamp", default=datetime.datetime.now().strftime('%Y-%m-%d'),
                    help="Timestamp to use in the output directory name")
parser.add_argument("--dry-run", action="store_true", help="Print commands without running them")

args = parser.parse_args()

print("TIMESTAMP:", args.timestamp)


for key in "hg38_reference_fasta", "gencode_gtf", "variation_clusters_bed", "aou1027_annotations", "hprc256_annotations", "tenk10k_annotations":
    if (key == "aou1027_annotations" and args.skip_aou1027_annotations
        ) or (key == "variation_clusters_bed" and args.skip_variation_cluster_annotations
        ) or (key == "hprc256_annotations" and args.skip_hprc256_annotations
        ) or (key == "tenk10k_annotations" and args.skip_tenk10k_annotations):

        setattr(args, key, None)
        continue
    path = getattr(args, key)
    if not file_exists(path):
        parser.error(f"{key} file not found {path}")

    if path.startswith("gs://"):
        run(f"gsutil -m cp -n {path} .")
        path = os.path.basename(path)

    setattr(args, key, os.path.abspath(path))

base_dir = os.path.abspath(".")
working_dir = os.path.abspath(f"results__{args.timestamp}")

run(f"mkdir -p {working_dir}")
chdir(working_dir)

# create a release draft folder
release_draft_folder = os.path.abspath(f"release_draft_{args.timestamp}")
run(f"mkdir -p {release_draft_folder}")

log_file_path = f"{release_draft_folder}/{args.output_prefix}.all_steps.log"
print(f"Writing to log file: {log_file_path}")
tee_stdout_and_stderr_to_log_file(log_file_path)

script_start_time = time.time()
start_datetime = datetime.datetime.now()
print(f"Started at: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")

# STEP #1  (already done for hg38, and results are publicly available. Will just download them in step #2)
#git clone git@github.com:broadinstitute/colab-repeat-finder.git
#cd colab-repeat-finder/python
#python3 perfect_repeat_finder.py  --min-repeats 3  --min-span 9  --min-motif-size 1  --max-motif-size 1000  --output-prefix perfect_repeats.hg38  --show-progress-bar  ${REFERENCE_FASTA_PATH}


# STEP #2:  generate catalog

# list of source catalogs, in order. If a locus is defined in more than one catalog (ie. overlapping boundaries,
# same motif), then the definition in the catalog that's earlier in the list will take precedence over definitions in
# subsequent catalogs.
source_catalogs_in_order = [
    NS(name="TRExplorerV1:KnownDiseaseAssociatedLoci",      merge="v1", url="https://raw.githubusercontent.com/broadinstitute/str-analysis/69dd90ecbc1dcbb23d5ca84ab4022850a283114f/str_analysis/variant_catalogs/variant_catalog_without_offtargets.GRCh38.json"),
    NS(name="TRExplorerV1:Illumina174kPolymorphicTRs",      merge="v1", url="https://storage.googleapis.com/str-truth-set/hg38/ref/other/illumina_variant_catalog.sorted.bed.gz"),
    NS(name="TRExplorerV1:PerfectRepeatsInReference",       merge="v1", url="https://storage.googleapis.com/str-truth-set/hg38/ref/other/colab-repeat-finder/hg38_repeats.motifs_1_to_1000bp.repeats_3x_and_spans_9bp/hg38_repeats.motifs_1_to_1000bp.repeats_3x_and_spans_9bp.bed.gz"),
    NS(name="TRExplorerV1:PolymorphicTRsInT2TAssemblies",   merge="v1", url="https://storage.googleapis.com/str-truth-set-v2/filter_vcf/all_repeats_including_homopolymers_keeping_loci_that_have_overlapping_variants/combined/merged_expansion_hunter_catalog.78_samples.json.gz"),

    NS(name="TRExplorerV2:KnownDiseaseAssociatedLociV2",    merge="v2a", url="https://storage.googleapis.com/tandem-repeat-catalog/v2.0/known_disease_associated_loci_v2.loci_to_include_in_catalog.bed.gz"),
    NS(name="TRExplorerV2:PolymorphicTRsInT2TAssembliesV2", merge="v2a", url="https://storage.googleapis.com/str-truth-set-v2/filter_vcf_v2__2025_12_29/combined.321_catalogs.tandem_repeats.bed.gz"),
    NS(name="TRExplorerV2:KnownFunctionalVNTRs",            merge="v2a", url="https://storage.googleapis.com/tandem-repeat-catalog/v2.0/known_functional_VNTRs.loci_to_include_in_catalog.bed.gz"),

    NS(name="TRExplorerV2:HipSTRCatalog"        ,           merge="v2b", url="https://storage.googleapis.com/tandem-repeat-catalog/v2.0/hg38.hipstr_reference.loci_to_include_in_catalog.bed.gz"),
    NS(name="TRExplorerV2:AdottoTRsFromDanzi2025",          merge="v2b", url="https://storage.googleapis.com/tandem-repeat-catalog/v2.0/Danzi_2025.adotto.loci_to_include_in_catalog.bed.gz"),
    NS(name="TRExplorerV2:ClinvarIndelsThatAreTRs2025",     merge="v2b", url="https://storage.googleapis.com/tandem-repeat-catalog/v2.0/clinvar_2025_11_03.loci_to_include_in_catalog.bed.gz"),
    NS(name="TRExplorerV2:Tanudisastro2025",                merge="v2b", url="https://storage.googleapis.com/tandem-repeat-catalog/v2.0/Tanudisastro_2025.loci_to_include_in_catalog.bed.gz"),
    NS(name="TRExplorerV2:Manigbas2024",                    merge="v2b", url="https://storage.googleapis.com/tandem-repeat-catalog/v2.0/Manigbas_2024_genotyped_TRs.bed.gz"),
    NS(name="TRExplorerV2:Sulovari2021",                    merge="v2b", url="https://storage.googleapis.com/tandem-repeat-catalog/v2.0/Sulovari_2019_human_specific_STRs.bed.gz"),
    NS(name="TRExplorerV2:Garg2021",                        merge="v2b", url="https://storage.googleapis.com/tandem-repeat-catalog/v2.0/Garg_2021_eVNTRs_mVNTRs.bed.gz"),
    NS(name="TRExplorerV2:Mukamel2021",                     merge="v2b", url="https://storage.googleapis.com/tandem-repeat-catalog/v2.0/Mukamel_2021.loci_to_include_in_catalog.bed.gz"),
    NS(name="TRExplorerV2:Annear2021",                      merge="v2b", url="https://storage.googleapis.com/tandem-repeat-catalog/v2.0/Annear_2021.loci_to_include_in_catalog.bed.gz"),
    NS(name="TRExplorerV2:Gymrek2016",                      merge="v2b", url="https://storage.googleapis.com/tandem-repeat-catalog/v2.0/Gymrek_2016_eSTRs.tandem_repeats.bed.gz"),
    NS(name="TRExplorerV2:Hause2016",                       merge="v2b", url="https://storage.googleapis.com/tandem-repeat-catalog/v2.0/Hause_2016_cancer_MSI_loci.bed.gz"),
]

source_catalog_paths = {}
for source_catalog in source_catalogs_in_order:
    if not os.path.isfile(os.path.basename(source_catalog.url)):
        run(f"wget -O {os.path.basename(source_catalog.url)}.tmp -qnc {source_catalog.url} && mv {os.path.basename(source_catalog.url)}.tmp {os.path.basename(source_catalog.url)}")
    source_catalog_paths[source_catalog.name] = os.path.abspath(os.path.basename(source_catalog.url))

# preprocess catalog of known disease-associated loci: split compound definitions
run(f"python3 -u -m str_analysis.split_adjacent_loci_in_expansion_hunter_catalog {source_catalog_paths['TRExplorerV1:KnownDiseaseAssociatedLoci']}", step_number=1)
source_catalog_paths['TRExplorerV1:KnownDiseaseAssociatedLoci'] = source_catalog_paths['TRExplorerV1:KnownDiseaseAssociatedLoci'].replace(".json", ".split.json")
# change motif definition for the RFC1 locus from AARRG => AAAAG since our catalog doesn't currently support IUPAC codes
run(f"sed -i 's/AARRG/AAAAG/g' {source_catalog_paths['TRExplorerV1:KnownDiseaseAssociatedLoci']}", step_number=2)
run(f"gzip -f {source_catalog_paths['TRExplorerV1:KnownDiseaseAssociatedLoci']}", step_number=3)

source_catalog_paths['TRExplorerV1:KnownDiseaseAssociatedLoci'] += ".gz"

if not args.dry_run:
    # compute stats for primary disease-associated loci
    with gzip.open(source_catalog_paths["TRExplorerV1:KnownDiseaseAssociatedLoci"]) as f:
        known_disease_associated_loci = json.load(f)

    primary_disease_associated_loci = [
        x for x in known_disease_associated_loci if x["Diseases"] and (
            x["LocusId"].startswith("HOXA") or x["LocusId"].startswith("ARX") or "_" not in x["LocusId"]
        )
    ]


    # check the number of primary disease-associated loci, not counting adjacent repeats and historic candidate loci that
    # are not currently considered monogenic
    if len(primary_disease_associated_loci) != 63:
        raise ValueError(f"Expected 63 primary disease-associated loci, found {len(primary_disease_associated_loci)}")

primary_disease_associated_loci_path = source_catalog_paths["TRExplorerV1:KnownDiseaseAssociatedLoci"].replace(
    ".json.gz", ".primary_disease_associated_loci.json.gz")

if not args.dry_run:
    with gzip.open(primary_disease_associated_loci_path, "wt") as f:
        json.dump(primary_disease_associated_loci, f, indent=4)

    run(f"""python3 -u -m str_analysis.annotate_and_filter_str_catalog \
        --verbose \
        --reference-fasta {args.hg38_reference_fasta} \
        --min-interval-size-bp 1 \
        --skip-gene-annotations \
        --skip-mappability-annotations \
        --skip-disease-loci-annotations \
        --discard-loci-with-non-ACGT-bases-in-reference \
        --discard-loci-with-non-ACGTN-bases-in-motif \
        --output-path {primary_disease_associated_loci_path} \
        {primary_disease_associated_loci_path}""", step_number=4)

    run(f"python3 -m str_analysis.compute_catalog_stats --reference-fasta {args.hg38_reference_fasta} --verbose {primary_disease_associated_loci_path}", step_number=5)

adjacent_repeats_source_bed = None
for motif_size_label, min_motif_size, max_motif_size, release_tar_gz_path in [
    ("1_to_1000bp_motifs",  1, 1000, None),
    #("2_to_1000bp_motifs",  2, 1000, f"{args.output_prefix}.subset.all_loci_except_homopolymers.tar.gz"),
    #("homopolymers",        1, 1,    f"{args.output_prefix}.subset.only_homopolymer_loci.tar.gz"),
    #("2_to_6bp_motifs",     2, 6,    f"{args.output_prefix}.subset.only_loci_with_2_to_6bp_motifs.tar.gz"),
    #("7_to_1000bp_motifs",  7, 1000, f"{args.output_prefix}.subset.only_loci_with_7_to_1000bp_motifs.tar.gz"),
]:

    if (args.start_with_step and args.start_with_step > 2) and motif_size_label != "1_to_1000bp_motifs":
        continue

    print("="*200)
    chdir(working_dir)
    run(f"mkdir -p {motif_size_label}")
    chdir(motif_size_label)

    output_prefix = os.path.abspath(f"{args.output_prefix}.{motif_size_label}")

    filtered_source_catalog_paths = {}
    for i, source_catalog in enumerate(source_catalogs_in_order):
        catalog_path = source_catalog_paths[source_catalog.name]

        if catalog_path.endswith(".json.gz"):
            filtered_catalog_path = catalog_path.replace(".json.gz", ".filtered.json.gz")
        elif catalog_path.endswith(".bed.gz"):
            filtered_catalog_path = catalog_path.replace(".bed.gz", ".filtered.json.gz")
        else:
            raise ValueError(f"Unexpected file extension for {catalog_path}")

        filtered_catalog_path = os.path.abspath(os.path.basename(filtered_catalog_path))
        filtered_source_catalog_paths[source_catalog.name] = filtered_catalog_path

        extra_args = ""
        if source_catalog.name == "TRExplorerV2:PolymorphicTRsInT2TAssembliesV2":
            extra_args = f"--min-repeats-in-reference 1 "

        run(f"""python3 -u -m str_analysis.annotate_and_filter_str_catalog \
            --verbose \
            --known-disease-associated-loci {primary_disease_associated_loci_path} \
            --reference-fasta {args.hg38_reference_fasta} \
            {extra_args} \
            --min-motif-size {min_motif_size} \
            --max-motif-size {max_motif_size} \
            --min-interval-size-bp 1 \
            --skip-gene-annotations \
            --skip-mappability-annotations \
            --skip-disease-loci-annotations \
            --set-locus-id \
            --discard-loci-with-non-ACGT-bases-in-reference \
            --discard-loci-with-non-ACGTN-bases-in-motif \
            --output-path {filtered_catalog_path} \
            {catalog_path}""",
            step_number=6)

        print(f"Stats for {catalog_path}")
        run(f"python3 -m str_analysis.compute_catalog_stats "
            f"--reference-fasta {args.hg38_reference_fasta} "
            f"--verbose "
            f"{filtered_catalog_path}",
            step_number=7)

    # NOTE: we don't use the --merge-adjacent-loci-with-same-motif option for str_analysis.merge_loci because
    # it's important to preserve locus definitions as they appear in the individual source catalogs. If loci
    # are merged together, this can create incompatibility with loci in source catalogs (ie. the illumina catalog) or
    # between future versions of the overall Simple Repeat Catalog since newly-added loci could merge with previously
    # added loci, causing loss of those loci from new versions of the catalog. Variation cluster analysis is a better
    # way to merge adjacent loci where needed.
    source_catalog_paths_for_merge_command_v1 = " ".join([
        f"{source_catalog.name}:{filtered_source_catalog_paths[source_catalog.name]}"
        for source_catalog in source_catalogs_in_order if source_catalog.merge == "v1"
    ])

    run(f"""python3 -u -m str_analysis.merge_loci --verbose \
        --add-found-in-fields \
        --output-format JSON \
        --discard-extra-fields-from-input-catalogs \
        --overlap-fraction 0.66 \
        --overlapping-loci-action keep-first \
        --write-merge-stats-tsv \
        --write-outer-join-table \
        --write-bed-files-with-unique-loci \
        --outer-join-overlap-table-min-sources 1 \
        --output-prefix {output_prefix}.merged_v1 \
        {source_catalog_paths_for_merge_command_v1}""", step_number=8)

    run(f"""python3 -u -m str_analysis.annotate_and_filter_str_catalog --verbose \
        --reference-fasta {args.hg38_reference_fasta} \
        --min-motif-size {min_motif_size} \
        --max-motif-size {max_motif_size} \
        --min-interval-size-bp 1 \
        --discard-overlapping-intervals-with-similar-motifs \
        --output-path {output_prefix}.merged_v1.annotated.json.gz \
        {output_prefix}.merged_v1.json.gz""", step_number=9)

    source_catalog_paths_for_merge_command_v2a = " ".join([
        f"merged:{output_prefix}.merged_v1.annotated.json.gz",
    ] + [
        f"{source_catalog.name}:{filtered_source_catalog_paths[source_catalog.name]}"
        for source_catalog in source_catalogs_in_order if source_catalog.merge == "v2a"
    ])

    run(f"""python3 -u -m str_analysis.merge_loci --verbose \
        --add-found-in-fields \
        --output-format JSON \
        --overlapping-loci-action keep-first \
        --write-merge-stats-tsv \
        --overlap-fraction 0.66 \
        --motif-length-match-sufficient-for-VNTRs \
        --min-jaccard-similarity 0.66 \
        --output-prefix {output_prefix}.merged_v2a \
        {source_catalog_paths_for_merge_command_v2a}""", step_number=10)

    run(f"""python3 -u -m str_analysis.annotate_and_filter_str_catalog --verbose \
        --reference-fasta {args.hg38_reference_fasta} \
        --min-motif-size {min_motif_size} \
        --max-motif-size {max_motif_size} \
        --min-interval-size-bp 1 \
        --output-path {output_prefix}.merged_v2a.annotated.json.gz \
        {output_prefix}.merged_v2a.json.gz""", step_number=11)

    source_catalog_paths_for_merge_command_v2b = " ".join([
       f"merged:{output_prefix}.merged_v2a.annotated.json.gz",
    ] + [
       f"{source_catalog.name}:{filtered_source_catalog_paths[source_catalog.name]}"
        for source_catalog in source_catalogs_in_order if source_catalog.merge == "v2b"
    ])

    run(f"""python3 -u -m str_analysis.merge_loci --verbose \
        --add-found-in-fields \
        --output-format JSON \
        --overlapping-loci-action keep-first \
        --write-merge-stats-tsv \
        --overlap-fraction 0.66 \
        --motif-length-match-sufficient-for-VNTRs \
        --min-jaccard-similarity 0.2 \
        --output-prefix {output_prefix}.merged_v2b \
        {source_catalog_paths_for_merge_command_v2b}""", step_number=12)

    run(f"""python3 -u -m str_analysis.annotate_and_filter_str_catalog --verbose \
        --reference-fasta {args.hg38_reference_fasta} \
        --min-motif-size {min_motif_size} \
        --max-motif-size {max_motif_size} \
        --min-interval-size-bp 1 \
        --output-path {output_prefix}.merged_v2b.annotated.json.gz \
        {output_prefix}.merged_v2b.json.gz""", step_number=13)

    source_catalog_paths_for_final_merge_command = " ".join([
        f"merged_v1:{output_prefix}.merged_v1.annotated.json.gz",
        f"merged_v2a:{output_prefix}.merged_v2a.annotated.json.gz",
        f"merged_v2b:{output_prefix}.merged_v2b.annotated.json.gz",
    ])

    run(f"""python3 -u -m str_analysis.merge_loci --verbose \
        --add-found-in-fields \
        --output-format JSON \
        --overlap-fraction 0.66 \
        --min-jaccard-similarity 0.66 \
        --motif-length-match-sufficient-for-VNTRs \
        --only-compare-loci-from-different-catalogs \
        --overlapping-loci-action keep-first \
        --write-merge-stats-tsv \
        --output-prefix {output_prefix}.final_merged \
        {source_catalog_paths_for_final_merge_command}""", step_number=14)

    annotated_catalog_path = f"{output_prefix}.EH.with_annotations.json.gz"
    run(f"""python3 -u -m str_analysis.annotate_and_filter_str_catalog --verbose \
        --reference-fasta {args.hg38_reference_fasta} \
        --gene-models-source gencode \
        --gene-models-source refseq \
        --gene-models-source mane \
        --known-disease-associated-loci {primary_disease_associated_loci_path} \
        --min-motif-size {min_motif_size} \
        --max-motif-size {max_motif_size} \
        --min-interval-size-bp 1 \
        --output-path {annotated_catalog_path} \
        {output_prefix}.final_merged.json.gz""", step_number=15)

    # create a version of the ExpansionHunter catalog without extra annotations
    run(f"""python3 << EOF
import gzip, ijson, json

f = gzip.open("{annotated_catalog_path}", "rt")
out = gzip.open("{output_prefix}.EH.json.gz", "wt")
i = 0
total = 0
out.write("[")
for record in ijson.items(f, "item", use_float=True):
    total += 1
    # skip chrM loci because ExpansionHunter prints an error like: 'Unable to extract chrM:-793-207 from hg38.fa'
    is_chrM = record["LocusId"].startswith("M-") or record["LocusId"].startswith("chrM-")
    if is_chrM: print(f"Skipping chrM locus: {{record['LocusId']}}")
    if is_chrM: continue
    if record["NsInFlanks"] > 5: continue
    if i > 0: out.write(", ")
    i += 1
    out.write(json.dumps({{ 
        k: v for k, v in record.items() if k in {{"LocusId", "ReferenceRegion", "VariantType", "LocusStructure"}} 
    }}, indent=4))
out.write("]")
print(f"Wrote {{i:,d}} out of {{total:,d}} ({{i/total:0.1%}}%) loci to {output_prefix}.EH.json.gz")
EOF
""", step_number=16)

    run(f"python3 -m str_analysis.filter_out_loci_with_Ns_in_flanks "
        f"-R {args.hg38_reference_fasta} "
        f"-o {output_prefix}.EH.without_loci_with_Ns_in_flanks.json.gz "
        f"--output-list-of-filtered-loci {output_prefix}.loci_with_Ns_in_flanks.txt "
        f"{output_prefix}.EH.json.gz", step_number=17)
    run(f"mv {output_prefix}.EH.without_loci_with_Ns_in_flanks.json.gz {output_prefix}.EH.json.gz", step_number=18)

    release_files = [
        f"{output_prefix}.bed.gz",
        f"{output_prefix}.bed.gz.tbi",
        f"{annotated_catalog_path}",
        f"{output_prefix}.EH.json.gz",
        f"{output_prefix}.TRGT.bed",
        f"{output_prefix}.LongTR.bed",
        f"{output_prefix}.HipSTR.bed",
        f"{output_prefix}.GangSTR.bed",
    ]


    # add variation cluster annotations to the catalog
    if args.variation_clusters_bed and not args.skip_variation_cluster_annotations:
        variation_clusters_and_isolated_TRs_release_filename = f"{args.variation_clusters_output_prefix}.TRGT.bed.gz"

        run(f"""python3 {base_dir}/scripts/add_variation_cluster_annotations_to_catalog.py \
            --verbose \
            --output-catalog-json-path {output_prefix}.EH.with_annotations.with_variation_clusters.json.gz \
            --known-pathogenic-loci-json-path {source_catalog_paths['TRExplorerV1:KnownDiseaseAssociatedLoci']} \
            {args.variation_clusters_bed} \
            {annotated_catalog_path}""", step_number=19)

        run(f"mv {output_prefix}.EH.with_annotations.with_variation_clusters.json.gz {annotated_catalog_path}", step_number=20)

        run(f"""python3 {base_dir}/scripts/generate_TRGT_catalog_with_isolated_repeats_and_variation_clusters.py \
            -o {variation_clusters_and_isolated_TRs_release_filename} \
            {args.variation_clusters_bed} \
            {annotated_catalog_path}""", step_number=21)

        release_files.append(variation_clusters_and_isolated_TRs_release_filename)

        run(f"python3 {base_dir}/scripts/convert_trgt_catalog_to_longtr_format.py {variation_clusters_and_isolated_TRs_release_filename}",
            step_number=22)

        release_files.append(variation_clusters_and_isolated_TRs_release_filename.replace(".TRGT.bed.gz", ".LongTR.bed.gz"))

    # add allele frequencies to the catalog
    run(f"""python3 -u {base_dir}/scripts/add_allele_frequency_annotations.py \
            --add-t2t-assembly-frequencies-to-overlapping-loci \
            -o {annotated_catalog_path}.with_allele_frequencies.json.gz  {annotated_catalog_path}""", step_number=23)

    run(f"mv {annotated_catalog_path}.with_allele_frequencies.json.gz {annotated_catalog_path}", step_number=24)

    # add AoU1027 annotations
    if args.aou1027_annotations:
        run(f"""python3 {base_dir}/scripts/add_AoU_annotations_to_catalog.py \
            --output-catalog-json-path {output_prefix}.EH.with_annotations.with_AoU_annotations.json.gz \
            --known-pathogenic-loci-json-path {source_catalog_paths['TRExplorerV1:KnownDiseaseAssociatedLoci']} \
            {args.aou1027_annotations} \
            {annotated_catalog_path}""", step_number=25)

        run(f"mv {output_prefix}.EH.with_annotations.with_AoU_annotations.json.gz {annotated_catalog_path}", step_number=26)

    # TODO: add HPRC256 and TenK10K annotations?

    # annotate with "TRsInRegion" based on adjacent loci
    if motif_size_label == "1_to_1000bp_motifs":
        adjacent_repeats_source_bed = f"{output_prefix}.bed.gz"

    # convert to BED
    run(f"python3 -m str_analysis.convert_expansion_hunter_catalog_to_bed --split-adjacent-repeats "
        f"{annotated_catalog_path}  --output-file {output_prefix}.bed.gz", step_number=27)

    run(f"python3 -m str_analysis.add_adjacent_loci_to_expansion_hunter_catalog "
        f"--ref-fasta {args.hg38_reference_fasta} "
        f"--source-of-adjacent-loci {adjacent_repeats_source_bed} "
        f"--add-extra-field TRsInRegion "
        f"--only-add-extra-fields "
        f"-o {annotated_catalog_path}.with_adjacent_loci_annotation.json.gz "
        f"{annotated_catalog_path}", step_number=28)

    run(f"mv {annotated_catalog_path}.with_adjacent_loci_annotation.json.gz {annotated_catalog_path}", step_number=29)


    # convert to TSV
    run(f"""python3 << EOF
import gzip
import json
import pandas as pd
from pprint import pformat
with gzip.open("{annotated_catalog_path}", "rt") as f:
    data = json.load(f)
print(f"Loaded {{len(data):,d}} records from {annotated_catalog_path}")
print("Writing it to TSV file...")
df = pd.DataFrame(data)
core_columns = [
    'LocusId', 'ReferenceRegion', 'LocusStructure', 'CanonicalMotif', 'TRsInRegion',
    'Source', 'GencodeGeneRegion', 'GencodeGeneId', 'GencodeGeneName', 'GencodeTranscriptId',
    'RefseqGeneRegion', 'RefseqGeneId', 'RefseqGeneName', 'RefseqTranscriptId', 
    'ManeGeneRegion', 'ManeGeneId', 'ManeGeneName', 'ManeTranscriptId',
    'KnownDiseaseAssociatedMotif',  'KnownDiseaseAssociatedLocus', 'NsInFlanks',
    'LeftFlankMappability', 'FlanksAndLocusMappability', 'RightFlankMappability', 
    'FoundInKnownDiseaseAssociatedLoci', 'FoundInIllumina174kPolymorphicTRs', 
    'FoundInPerfectRepeatsInReference', 'FoundInPolymorphicTRsInT2TAssemblies', 
    'NumRepeatsInReference', 'ReferenceRepeatPurity', 
    'AlleleFrequenciesFromIllumina174k', 'StdevFromIllumina174k',
    'AlleleFrequenciesFromT2TAssemblies', 'StdevFromT2TAssemblies',
    'VariationCluster', 'VariationClusterSizeDiff',
    #'AlleleFrequenciesFromHPRC256', 'StdevFromHPRC256',
    'StdevFromAoU1027', 'MedianAlleleFromAoU1027', 'MaxAlleleFromAoU1027', 'NumUniqueAllelesFromAoU1027',
]

drop_columns = ['VariantType', ]
for c in set(core_columns)  - set(df.columns): df[c] = None
df = df[core_columns + [c for c in df.columns if c not in (core_columns + drop_columns)]]

output_tsv_path = "{annotated_catalog_path.replace('.json.gz', '') + '.tsv.gz'}"
df.to_csv(output_tsv_path, sep="\\t", index=False)
print(f"Wrote {{len(df):,d}} rows to {{output_tsv_path}} with columns: {{pformat(list(df.columns))}}")
EOF
""", step_number=30)

    # convert the catalog from ExpansionHunter catalog format to TRGT, LongTR, HipSTR, and GangSTR formats
    run(f"python3 -m str_analysis.convert_expansion_hunter_catalog_to_trgt_catalog -R {args.hg38_reference_fasta} --split-adjacent-repeats {annotated_catalog_path}  --output-file {output_prefix}.TRGT.bed", step_number=31)
    run(f"python3 -m str_analysis.convert_expansion_hunter_catalog_to_longtr_format  {annotated_catalog_path}  --output-file {output_prefix}.LongTR.bed", step_number=32)
    run(f"python3 -m str_analysis.convert_expansion_hunter_catalog_to_hipstr_format  {annotated_catalog_path}  --output-file {output_prefix}.HipSTR.bed", step_number=33)
    run(f"python3 -m str_analysis.convert_expansion_hunter_catalog_to_gangstr_spec   {annotated_catalog_path}  --output-file {output_prefix}.GangSTR.bed", step_number=34)

    # Confirm that the TRGT catalog passes 'trgt validate'
    run(f"trgt validate --genome {args.hg38_reference_fasta}  --repeats {output_prefix}.TRGT.bed", step_number=35)

    # Print missing values
    run(f"python3 {base_dir}/scripts/print_missing_values_percentages_in_json_or_tsv.py  {annotated_catalog_path}", step_number=36)

    # Perform basic internal consistency checks on the JSON catalog
    run(f"python3 {base_dir}/scripts/validate_catalog.py " +
        f"--known-pathogenic-loci-json-path {source_catalog_paths['TRExplorerV1:KnownDiseaseAssociatedLoci']} " +
        ("--check-for-presence-of-annotations --check-for-presence-of-all-known-loci --check-for-presence-of-all-loci-from-v1 " if motif_size_label == "1_to_1000bp_motifs" else "") +
        f"{annotated_catalog_path}", step_number=37)

    # copy files to the release_draft folder and compute catalog stats
    updated_release_files = []
    for path in release_files:
        if path.endswith(".bed"):
            if not os.path.isfile(f"{path}.gz"):
                if path.endswith(".TRGT.bed"):
                    run(f"gzip -f {path}", step_number=38)  # TRGT v1.1.1 and lower only works with gzip, not bgzip
                else:
                    run(f"bgzip -f {path}", step_number=39)
            updated_release_files.append(f"{path}.gz")
        else:
            if path.endswith(".json") or path.endswith(".json.gz") and ".EH." in path:
                run(f"python3 {base_dir}/scripts/validate_json.py -k LocusId -k LocusStructure -k ReferenceRegion -k VariantType {path}", step_number=40)
            updated_release_files.append(path)

    if release_tar_gz_path is None:
        for path in updated_release_files:
            run(f"cp {path} {release_draft_folder}", step_number=41)
    else:
        run(f"tar czf {release_tar_gz_path} -C {os.path.dirname(output_prefix)} " + " ".join([os.path.basename(p) for p in updated_release_files]), step_number=42)
        run(f"cp {release_tar_gz_path} {release_draft_folder}", step_number=43)

    run(f"python3 -m str_analysis.compute_catalog_stats --reference-fasta {args.hg38_reference_fasta} --verbose {annotated_catalog_path}", step_number=44)

    # report hours, minutes, seconds relative to script_start_time
    diff = time.time() - script_start_time
    print(f"Done generating {output_prefix} catalog. Took {diff//3600:.0f}h, {(diff%3600)//60:.0f}m, {diff%60:.0f}s")

    if motif_size_label != "1_to_1000bp_motifs":
        continue

    # compare to the GangSTR_v17 catalog to make sure it's included
    comparison_catalogs_in_order = [
        ("GangSTR_v17", "https://s3.amazonaws.com/gangstr/hg38/genomewide/hg38_ver17.bed.gz"),
        #("vamos_catalog_v2.1", "https://storage.googleapis.com/str-truth-set/hg38/ref/other/vamos_catalog.v2.1.bed.gz"),
    ]

    comparison_catalog_paths = {}
    for catalog_name, url in comparison_catalogs_in_order:
        if not os.path.isfile(os.path.basename(url)):
            run(f"wget -O {os.path.basename(url)}.tmp -qnc {url} && mv {os.path.basename(url)}.tmp {os.path.basename(url)}")
        comparison_catalog_paths[catalog_name] = os.path.abspath(os.path.basename(url))

    path_after_conversion = comparison_catalog_paths["GangSTR_v17"].replace(".bed.gz", ".json.gz")
    run(f"python3 -u -m str_analysis.convert_gangstr_spec_to_expansion_hunter_catalog --verbose {comparison_catalog_paths['GangSTR_v17']} -o {path_after_conversion}", step_number=45)
    comparison_catalog_paths["GangSTR_v17"] = path_after_conversion

    # compare catalog to other catalogs
    start_time = time.time()

    for catalog_name, path in comparison_catalog_paths.items():
        filtered_comparison_catalog_path = re.sub("(.json|.bed)(.gz)?$", "", path) + ".filtered.json.gz"

        run(f"""python3 -m str_analysis.annotate_and_filter_str_catalog \
            --reference-fasta {args.hg38_reference_fasta} \
            --skip-gene-annotations \
            --skip-mappability-annotations \
            --skip-disease-loci-annotations \
            --min-motif-size {min_motif_size} \
            --max-motif-size {max_motif_size} \
            --output-path {filtered_comparison_catalog_path} \
            --verbose \
            {path}""", step_number=46)

        run(f"python3 -m str_analysis.compute_catalog_stats --reference-fasta {args.hg38_reference_fasta} --verbose {filtered_comparison_catalog_path}", step_number=47)

        run(f"""python3 -u -m str_analysis.merge_loci \
            --output-prefix {catalog_name} \
            --output-format JSON \
            --overlapping-loci-action keep-first \
            --verbose \
            --write-merge-stats-tsv \
            {annotated_catalog_path} \
            {filtered_comparison_catalog_path}""", step_number=48)

    diff = time.time() - start_time
    print(f"Done with comparisons. Took {diff//3600:.0f}h, {(diff%3600)//60:.0f}m, {diff%60:.0f}s")

# Print final timing summary
end_datetime = datetime.datetime.now()
total_elapsed_time = time.time() - script_start_time
print("\n" + "="*80)
print(f"Started at:  {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"Finished at: {end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"Total time:  {total_elapsed_time//3600:.0f}h, {(total_elapsed_time%3600)//60:.0f}m, {total_elapsed_time%60:.0f}s")
print("="*80)

